{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d3d14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git -q\n",
    "!pip install bitsandbytes datasets accelerate loralib -q\n",
    "# !pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu11.8/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fac15894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766d71b4bfa94edcba79e02062f344a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "739ea420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from accelerate import Accelerator\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, RobertaTokenizer, GPT2Tokenizer, \\\n",
    "    GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "# torch.cuda.set_device(0)\n",
    "# # torch.cuda.set_device(1)\n",
    "# # torch.cuda.set_device(2)\n",
    "# # torch.cuda.set_device(3)\n",
    "# torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e0129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69089fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "ds = load_dataset(\"nvidia/HelpSteer2\")\n",
    "train = ds['train'] # len(train) = 20324 (95%)\n",
    "val = ds['validation']     # len(val) = 1038 (5%)\n",
    "\n",
    "train_pd = pd.DataFrame(train)\n",
    "val_pd = pd.DataFrame(val)\n",
    "train_pd.head()\n",
    "val_pd.head()\n",
    "\n",
    "train_df = train_pd.copy()\n",
    "val_df = val_pd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6129f",
   "metadata": {},
   "source": [
    "## 1. no preprocessing needed for llm , lm preprocessing for lstm: [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c678645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = train_pd[:500]\n",
    "val_pd = val_pd[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5171e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length of prompts and response:  548 1045\n",
      "Vocabulary size from train prompt + response  7095\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Tokenize the prompts and answers\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_pd['prompt'].tolist() + train_pd['response'].tolist()),\n",
    "                                  specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Convert texts to sequences\n",
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "train_prompt_sequences = [text_pipeline(text) for text in train_pd['prompt']]\n",
    "train_answer_sequences = [text_pipeline(text) for text in train_pd['response']]\n",
    "val_prompt_sequences = [text_pipeline(text) for text in val_pd['prompt']]\n",
    "val_answer_sequences = [text_pipeline(text) for text in val_pd['response']]\n",
    "\n",
    "max_len_prompt = max(len(seq) for seq in train_prompt_sequences)\n",
    "max_len_answer = max(len(seq) for seq in train_answer_sequences)\n",
    "print('max token length of prompts and response: ', max_len_prompt, max_len_answer)\n",
    "# Vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "print('Vocabulary size from train prompt + response ', vocab_size)\n",
    "\n",
    "# Pad the sequences\n",
    "def pad_sequences(sequences, max_len):\n",
    "    return pad_sequence([torch.tensor(seq[:max_len]) for seq in sequences], batch_first=True,\n",
    "                        padding_value=vocab[\"<unk>\"])\n",
    "\n",
    "train_prompt_sequences = pad_sequences(train_prompt_sequences, max_len_prompt)\n",
    "train_answer_sequences = pad_sequences(train_answer_sequences, max_len_answer)\n",
    "val_prompt_sequences = pad_sequences(val_prompt_sequences, max_len_prompt)\n",
    "val_answer_sequences = pad_sequences(val_answer_sequences, max_len_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777b6a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300, 548]),\n",
       " torch.Size([300, 1045]),\n",
       " torch.Size([50, 387]),\n",
       " torch.Size([50, 808]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompt_sequences.shape, train_answer_sequences.shape, val_prompt_sequences.shape, val_answer_sequences.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e8b088",
   "metadata": {},
   "source": [
    "Not sure why seq length isnt same for train and val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fa6c8",
   "metadata": {},
   "source": [
    "## 2.Explain and implement how LLM (2B to 8B parameters) over simple LM can help improve the system accuracy. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12573dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 2 Seq LM using lstm on Pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, hidden):\n",
    "        encoder_embedded = self.embedding(encoder_input)\n",
    "        encoder_output, (hidden, cell) = self.encoder_lstm(encoder_embedded, hidden)\n",
    "        \n",
    "        decoder_embedded = self.embedding(decoder_input)\n",
    "        decoder_output, (hidden, cell) = self.decoder_lstm(decoder_embedded, (hidden, cell))\n",
    "        \n",
    "        output = self.fc(decoder_output)\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "embedding_dim = 128\n",
    "hidden_units = 256\n",
    "\n",
    "model = Seq2Seq(vocab_size, embedding_dim, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9b8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<unk>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device='cpu' # using cpu since gpu gives memory error\n",
    "# Move model to GPU\n",
    "# device_ids = [0, 1, 2, 3]  # Replace with desired GPU IDs\n",
    "# model = nn.DataParallel(model, device_ids=device_ids)\n",
    "# model.to(f'cuda:{device_ids[0]}')  # Move the model to the first GPU\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # input_tensor = input_tensor.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# Prepare the target data for the decoder (shifted one time step) !!!\n",
    "def prepare_target_sequences(sequences):\n",
    "    inputs = sequences[:, :-1]\n",
    "    targets = sequences[:, 1:]\n",
    "    return inputs, targets\n",
    "   \n",
    "train_answer_sequences_input, train_answer_sequences_output = prepare_target_sequences(train_answer_sequences)\n",
    "val_answer_sequences_input, val_answer_sequences_output = prepare_target_sequences(val_answer_sequences)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "train_prompt_sequences = train_prompt_sequences.to(device)\n",
    "train_answer_sequences_input = train_answer_sequences_input.to(device)\n",
    "train_answer_sequences_output = train_answer_sequences_output.to(device)\n",
    "val_prompt_sequences = val_prompt_sequences.to(device)\n",
    "val_answer_sequences_input = val_answer_sequences_input.to(device)\n",
    "val_answer_sequences_output = val_answer_sequences_output.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6819aee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 8.87558364868164\n",
      "Epoch [2/50], Loss: 8.849912643432617\n",
      "Epoch [3/50], Loss: 8.822985649108887\n",
      "Epoch [4/50], Loss: 8.792208671569824\n",
      "Epoch [5/50], Loss: 8.754260063171387\n",
      "Epoch [6/50], Loss: 8.703964233398438\n",
      "Epoch [7/50], Loss: 8.631867408752441\n",
      "Epoch [8/50], Loss: 8.517773628234863\n",
      "Epoch [9/50], Loss: 8.311432838439941\n",
      "Epoch [10/50], Loss: 7.934554100036621\n",
      "Epoch [11/50], Loss: 7.5272321701049805\n",
      "Epoch [12/50], Loss: 7.2270894050598145\n",
      "Epoch [13/50], Loss: 7.004649639129639\n",
      "Epoch [14/50], Loss: 6.82792854309082\n",
      "Epoch [15/50], Loss: 6.6866374015808105\n",
      "Epoch [16/50], Loss: 6.579782009124756\n",
      "Epoch [17/50], Loss: 6.508011341094971\n",
      "Epoch [18/50], Loss: 6.469689846038818\n",
      "Epoch [19/50], Loss: 6.459895610809326\n",
      "Epoch [20/50], Loss: 6.468969345092773\n",
      "Epoch [21/50], Loss: 6.484289169311523\n",
      "Epoch [22/50], Loss: 6.495165824890137\n",
      "Epoch [23/50], Loss: 6.496269226074219\n",
      "Epoch [24/50], Loss: 6.487391948699951\n",
      "Epoch [25/50], Loss: 6.471380233764648\n",
      "Epoch [26/50], Loss: 6.452081203460693\n",
      "Epoch [27/50], Loss: 6.432642936706543\n",
      "Epoch [28/50], Loss: 6.4148335456848145\n",
      "Epoch [29/50], Loss: 6.399575710296631\n",
      "Epoch [30/50], Loss: 6.3866286277771\n",
      "Epoch [31/50], Loss: 6.375542163848877\n",
      "Epoch [32/50], Loss: 6.36587381362915\n",
      "Epoch [33/50], Loss: 6.35689115524292\n",
      "Epoch [34/50], Loss: 6.3482255935668945\n",
      "Epoch [35/50], Loss: 6.339582920074463\n",
      "Epoch [36/50], Loss: 6.3308844566345215\n",
      "Epoch [37/50], Loss: 6.321742534637451\n",
      "Epoch [38/50], Loss: 6.312087059020996\n",
      "Epoch [39/50], Loss: 6.30206298828125\n",
      "Epoch [40/50], Loss: 6.291914463043213\n",
      "Epoch [41/50], Loss: 6.281908988952637\n",
      "Epoch [42/50], Loss: 6.272091865539551\n",
      "Epoch [43/50], Loss: 6.262415885925293\n",
      "Epoch [44/50], Loss: 6.2526750564575195\n",
      "Epoch [45/50], Loss: 6.242735862731934\n",
      "Epoch [46/50], Loss: 6.2325520515441895\n",
      "Epoch [47/50], Loss: 6.222198486328125\n",
      "Epoch [48/50], Loss: 6.211533546447754\n",
      "Epoch [49/50], Loss: 6.200573921203613\n",
      "Epoch [50/50], Loss: 6.1894450187683105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "466.6952950954437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "init = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output, _ = model(train_prompt_sequences, train_answer_sequences_input, None)\n",
    "#     loss = criterion(output.view(-1, vocab_size), train_answer_sequences_output.view(-1)) # ask\n",
    "    loss = criterion(output.reshape(-1, vocab_size), train_answer_sequences_output.reshape(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "time.time() - init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffedc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'lstm_lm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4e7e193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (embedding): Embedding(7095, 128)\n",
       "  (encoder_lstm): LSTM(128, 256, batch_first=True)\n",
       "  (decoder_lstm): LSTM(128, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=7095, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('lstm_lm_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "630d4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.111302852630615\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_output, _ = model(val_prompt_sequences, val_answer_sequences_input, None)\n",
    "#     val_loss = criterion(val_output.view(-1, vocab_size), val_answer_sequences_output.view(-1))\n",
    "    val_loss = criterion(val_output.reshape(-1, vocab_size), val_answer_sequences_output.reshape(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a76dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_lm_response(prompt, model, vocab, max_len_answer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompt_sequence = pad_sequence([torch.tensor(text_pipeline(prompt))[:max_len_prompt]], batch_first=True, padding_value=vocab[\"<unk>\"]).to(device)\n",
    "        decoder_input = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "        \n",
    "        hidden = None\n",
    "        decoded_sentence = []\n",
    "\n",
    "        for _ in range(max_len_answer):\n",
    "            output, hidden = model(prompt_sequence, decoder_input, hidden)\n",
    "            sampled_token_index = output.argmax(2)[:, -1].item()\n",
    "            sampled_word = vocab.lookup_token(sampled_token_index)\n",
    "            decoded_sentence.append(sampled_word)\n",
    "            if sampled_word == '<end>':\n",
    "                break\n",
    "            decoder_input = torch.cat([decoder_input, torch.tensor([[sampled_token_index]], dtype=torch.long).to(device)], dim=1)\n",
    "            \n",
    "        return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35e79aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give us creative product ideas related to air fresheners.\n",
      ", and , and , and , and , and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the\n"
     ]
    }
   ],
   "source": [
    "# Generate an answer - simple LM\n",
    "prompt_ = val_pd['prompt'][9]\n",
    "print(prompt_)\n",
    "print(generate_simple_lm_response(prompt_, model, vocab, max_len_answer))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee0c37a8",
   "metadata": {},
   "source": [
    "using lstm with limited training gives gibberish answers\n",
    "using LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad55b9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c073a14bc76a4b15a528404bf4b705b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name, add_bos_token=False, add_eos_token=True)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name)\n",
    "print(device)\n",
    "llm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c0e1da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please',\n",
       " 'Ä give',\n",
       " 'Ä us',\n",
       " 'Ä creative',\n",
       " 'Ä product',\n",
       " 'Ä ideas',\n",
       " 'Ä related',\n",
       " 'Ä to',\n",
       " 'Ä air',\n",
       " 'Ä fresh',\n",
       " 'eners',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90a1a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "meta lamma tokeniser output:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fresh', 'eners', '.']\n",
      "generated response:  Please give us creative product ideas related to air fresheners. I am looking for unique, new and innovative ideas for air fresheners. These ideas should be innovative and creative.\n"
     ]
    }
   ],
   "source": [
    "def generate_llm_responses(text_):\n",
    "    inputs = llm_tokenizer.encode_plus(text_, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                                 num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt_ = val_pd['prompt'][9]\n",
    "print('prompt: ', prompt_)\n",
    "print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b7972e3",
   "metadata": {},
   "source": [
    "clearly llm works better - implementation above, explaination - self attention etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59b6f6",
   "metadata": {},
   "source": [
    "## 3.\tExplain and implement how specific tokenizer and model can work well with the mentioned dataset. [1 Mark]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c623d25e",
   "metadata": {},
   "source": [
    "3 tokenisers - one comes directly with meta-lamma-3-8b, we use byte pair encoder tokeniser ((RobertaTokenizer) and one Character-Level Tokenizer and compare output of llm on all 3.\n",
    "\n",
    "first part is already done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0d118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe169d45364eca95d5b4c378592ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278c46a5a304331be40e70cffdaca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b73725720a6434495c6da363fc64f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62600a008844055b147594bde7c75ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58759c35d8874ad2a46c965b84fb1677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenized Prompts:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fres', 'hen', 'ers', '.']\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "bpe_tokenized_prompt_ = bpe_tokenizer.tokenize(prompt_)\n",
    "print(\"BPE Tokenized Prompts: \", bpe_tokenized_prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "157fb85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-Level Tokenized Prompts:  ['explain', 'master', 'slave', 'replication', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "char_tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "char_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "char_tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "vocab_size_=vocab_size\n",
    "# Train the tokenizer on the dataset\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=vocab_size_, special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "char_tokenizer.train_from_iterator(train_df['prompt'], trainer)\n",
    "\n",
    "# Tokenize using the character-level tokenizer\n",
    "char_tokenized_prompt_ = char_tokenizer.encode(prompt).tokens\n",
    "print(\"Character-Level Tokenized Prompts: \", char_tokenized_prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79412240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "bpe encoded response:  Please give us creative product ideas related to air fresheners. next's for is forï¿½ forï¿½ us. next's for is forï¿½ for's us. next's for is forï¿½ for with us. next's for is forï¿½ for The us. next's for is forï¿½ for was us. next's for is forï¿½ for \" us. next's for is forï¿½ for at us. next's for is forï¿½ for it us. next's for is forï¿½ forw us. next's for is forï¿½ for technology us. next's for is forï¿½ forE us. next's for is\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using BPE tokenizer\n",
    "inputs = bpe_tokenizer.encode_plus(prompt_, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                             num_return_sequences=1, pad_token_id=bpe_tokenizer.eos_token_id)\n",
    "response = bpe_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('prompt: ', prompt_)\n",
    "# print('\\n')\n",
    "print('bpe encoded response: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate predictions using BPE tokenizer\n",
    "# inputs = bpe_tokenizer.encode_plus(prompt_, return_tensors='pt', padding=True, truncation=True)\n",
    "# input_ids = inputs['input_ids']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "#                              num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "# response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print('prompt: ', prompt_)\n",
    "# print('\\n')\n",
    "# print('bpe encoded response: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "092513cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "\n",
      "\n",
      "char level encoded response:  Please give us creative product ideas related to air. to. like. to. like..........................................................\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using char tokenizer\n",
    "encoding = char_tokenizer.encode(prompt_)\n",
    "input_ids = torch.tensor([encoding.ids], dtype=torch.long).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)  # Dummy attention mask, since char_tokenizer doesn't provide it\n",
    "outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                             num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "response = char_tokenizer.decode(outputs[0].cpu().numpy().tolist())\n",
    "print('prompt: ', prompt_)\n",
    "print('\\n')\n",
    "print('char level encoded response: ', response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2f2e716",
   "metadata": {},
   "source": [
    "Clearly different tokeniser models give different op. clearly meta-lamma-3-8B inbuilt tokeniser works best for given model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b49f5",
   "metadata": {},
   "source": [
    "## 4.\tExplain and implement how sentiment analysis can be used to analyze user questions or contexts. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64650c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2a59c3",
   "metadata": {},
   "source": [
    "## 5.\tImplement fine-tuning for an open-source model to improve the results. [3 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94aede3",
   "metadata": {},
   "source": [
    "## 6.\tShow 5-10 working examples that show improvements of the accuracy of the fine-tuned model. [3 Marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9daee7e7",
   "metadata": {},
   "source": [
    "## 7.\tApply and fine-tune the generator model using Parameter-Efficient Fine-Tuning (PEFT). [3 Marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa4f98f",
   "metadata": {},
   "source": [
    "## 8.\tImplement a function to evaluate the model's performance using metrics such as accuracy and F1 score. [1 Mark]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916cf17",
   "metadata": {},
   "source": [
    "## 9.\tAnalyze the results to discuss the impact of PEFT on model performance and efficiency. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baeaca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
